---
title: "2022-11_FET_BM_Homework 1"
author: "Fabio Enrico Traverso"
date: "2022-11-07"
output: pdf_document
---
## Task 1

The setup is such that with the echo=TRUE option all the code will be printed.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
To perform the homework, the following packages are used:
```{r packages,include=TRUE,echo=TRUE , results=FALSE}
library(bayesm)
library(data.table)
library(knitr)
library(flextable)
library(tidyverse)
set.seed(2516)
set_flextable_defaults(fonts_ignore=TRUE)
library(compiler)
runireg_c=cmpfun(runireg) # a bit faster
```
To loop over the number of observations, I consructed a vector of increasing numbers. 
```{r task_1_code, include=TRUE,echo=TRUE , results=FALSE}
nobs <- c(5,10,100,1000,10000)

# storing matrices
b_b1 <- matrix(0, length(nobs), 3) # m rows, 3 columns
sd_b1 <- matrix(0, length(nobs), 3)
b_ols <- b_b1
sd_ols <- b_b1

iteration <- 0
for (n in nobs){
  print(n)
  iteration <- iteration + 1
  # Data Generating Process
  X <- cbind(rep(1,n),runif(n),runif(n))
  beta <- c(-1,3,6)
  y <- (X %*% beta + rnorm(n, mean = 0, sd = 1))# set up y
  
  # parameters of the simulation 
  R <- 80000 
  Mcmc1 <- list(R=R,keep=1, nprint = 0)
  Prior <- list(nu=0, ssq=0)
  
  # run regressions
  ## bayes
  out_b1 <- runireg_c(Data=list(y=y,X=X), Prior = Prior,Mcmc=Mcmc1)
  b_draw1 <-out_b1$betadraw 
  # for every sample size the mean and the standard deviation are computed 
  b_b1[iteration,] <- apply(b_draw1, 2, mean) 
  sd_b1[iteration,] <- apply(b_draw1, 2, sd) 
  
  ## ols
  out_ols <- lm(y ~ X[,-1])
  summary(out_ols)
  b_ols[iteration,] <- matrix(out_ols$coefficients, 1, 3)
  sd_ols[iteration,] <- matrix(sqrt(diag(vcov(out_ols))), 1, 3)
}
```

```{r task_1_res, include=TRUE,echo=FALSE , results='asis'}
res <- data.frame(n = nobs, b1 = b_b1[,1], sd_b1 = sd_b1[,1],
                            b2 = b_b1[,2], sd_b2 = sd_b1[,2],
                            b3 = b_b1[,3], sd_b3 = sd_b1[,3],
                            ols1 = b_ols[,1], sd_ols1 = sd_ols[,1],
                            ols2 = b_ols[,2], sd_ols2 = sd_ols[,2],
                            ols3 = b_ols[,3], sd_ols3 = sd_ols[,3])

round(res,4) %>% regulartable() %>% autofit() %>% fit_to_width(max_width = 6.5)
```
Both estimators converge to the true parameters, although the bayesian one presents a lower standard error, which implies it is more efficient than OLS.

## Task 2

We expect that due to multicollinearity, the OLS estimator is not feasible in this setup. 
In particular, the matrix $X'X$ is not invertible. However, the Bayesian estimator does not require the $X'X$ to be invertible and will likely estimate a virtually identical distribution for the two coefficients.
```{r task_2_code, include=TRUE,echo=TRUE , results=FALSE}
nobs <- 20000 
noby <- n 
b_b <- matrix(0, length(nobs), 3) 
sd_b <- matrix(0, length(nobs), 3)
b_ols <- b_b
sd_ols <- b_b

#Data Generating Process
hilf= runif(nobs)
X <- cbind(rep(1,n),hilf, hilf)
beta <- c(-1,3,6)
y <- (X %*% beta + rnorm(n, mean = 0, sd = 1))# set up y

# set up sampling
R <- 80000 # as in the lecture
Mcmc1 <- list(R=R,keep=1, nprint = 0)
Prior <- list(nu=0, ssq=0) # beta = 0 as default

# run regressions
## bayes
out_b <- runireg_c(Data=list(y=y,X=X), Prior = Prior,Mcmc=Mcmc1)
b_draw <-out_b$betadraw 
b_b[1,] <- apply(b_draw, 2, mean) # get mean for each coeff.
sd_b[1,] <- apply(b_draw, 2, sd) # standard deviation

## ols
out_ols <- lm(y ~ X[,-1]) # without intercept in x, is added automatically
summary(out_ols)
b_ols[1,] <- matrix(out_ols$coefficients, 1, 3)
sd_ols[1,] <- matrix(sqrt(diag(vcov(out_ols))), 1, 3)
```
As discussed above, multicollinearity is "solved" bz the program by eliminating one of the two regressors. 
```{r task_2_res, include=TRUE,echo=TRUE , results='asis'}
res <- data.frame(n = nobs, b1 = b_b[,1], sd_b1 = sd_b[,1],
                            b2 = b_b[,2], sd_b2 = sd_b[,2],
                            b3 = b_b[,3], sd_b3 = sd_b[,3],
                            ols1 = b_ols[,1], sd_ols1 = sd_ols[,1],
                            ols2 = b_ols[,2], sd_ols2 = sd_ols[,2],
                            ols3 = b_ols[,3], sd_ols3 = sd_ols[,3])

round(res,4) %>% regulartable() %>% autofit() %>% fit_to_width(max_width = 6.5)
kable(summary(b_draw), digits = 4)
kable(t(apply(b_draw,2,quantile)), digits = 4)
```
Finally, the distribution of the bayesian estimator confirms the hypotheses: 
the perfectly collinear regression coefficients' posterior distributions are 
indeed close to identical.


```{r task_2_plot, include=TRUE,echo=FALSE, Efig.show="hold"}
out.width="50%"
par(mfrow=c(1,3))
hist(b_draw[,1], breaks = 100)
hist(b_draw[,2], breaks = 100)
hist(b_draw[,3], breaks = 100)
dev.off()
```

## Task 3

```{r task_3_code, include=TRUE,echo=TRUE , results=FALSE}
nobs <- c(5,10,100,1000,10000) # different ns

# matrices in which results are stored
b_b3 <- matrix(0, length(nobs), 3) # m rows, 3 columns
sd_b3 <- matrix(0, length(nobs), 3)
b_pro <- b_b3
sd_pro <- b_b3

iteration <- 0

for (n in nobs){
  print(n)
  iteration <- iteration + 1
  # set up DGP
  #n <- 5
  X <- cbind(rep(1,n),runif(n),runif(n))
  beta <- c(-1,3,6)
  y <- (X %*% beta + rnorm(n, mean = 0, sd = 1))# set up y
  y <- ifelse(y<0,0,1)
  
  # set up sampling
  R <- 80000 # as in the lecture
  Mcmc1 <- list(R=R,keep=1, nprint = 0)
  #Prior <- list(nu=0, ssq=0) # beta = 0 as default
  
  # run regressions
  ## bayes
  out_b3 <- runireg_c(Data=list(y=y,X=X),Mcmc=Mcmc1)
  b_draw3 <-out_b3$betadraw 
  b_b3[iteration,] <- apply(b_draw3, 2, mean) # get mean for each coeff.
  sd_b3[iteration,] <- apply(b_draw3, 2, sd) # standard deviation
  
  ## ols
  out_pro <- glm(y ~ X[,-1], family=binomial(link="probit")) # without intercept in x, is added automatically
  summary(out_pro)
  b_pro[iteration,] <- matrix(out_pro$coefficients, 1, 3)
  sd_pro[iteration,] <- matrix(sqrt(diag(vcov(out_pro))), 1, 3)
}
```

Having access to linear observations is better than the probit ones. Note infact that the estimates in the first table show that the bayesian estimators for the probit data are not consistent, whereas the probit estimator does converge, although not fast, to the true parameters. The comparison in Table 2 also highlights that the bayes linear estimator is also converging to the true parameters, whereas the probit one does not. Hence, it is preferrable to have access to linear data. 


```{r task_3_res, include=TRUE,echo=FALSE , results='asis'}
res1 <- data.frame(n = nobs, b1 = b_b3[,1], sd_b1 = sd_b3[,1],
                            b2 = b_b3[,2], sd_b2 = sd_b3[,2],
                            b3 = b_b3[,3], sd_b3 = sd_b3[,3],
                            pro1 = b_pro[,1], sd_pro1 = sd_pro[,1],
                            pro2 = b_pro[,2], sd_pro2 = sd_pro[,2],
                            pro3 = b_pro[,3], sd_pro3 = sd_pro[,3])
res2 <- data.frame(n = nobs, b1.1 = b_b1[,1], sd_b1.1 = sd_b1[,1],
                            b2.1 = b_b1[,2], sd_b2.1 = sd_b1[,2],
                            b3.1 = b_b1[,3], sd_b3.1 = sd_b1[,3],
                            b1.3 = b_b3[,1], sd_b1.3 = sd_b3[,1],
                            b2.3 = b_b3[,2], sd_b2.3 = sd_b3[,2],
                            b3.3 = b_b3[,3], sd_b3.3 = sd_b3[,3])

round(res1, 4) %>% regulartable() %>%  colformat_num(digits=4)  %>% autofit() %>% fit_to_width(max_width = 6.5)
round(res2, 4) %>% regulartable() %>%  colformat_num(digits=4)  %>% autofit() %>% fit_to_width(max_width = 6.5)
```
